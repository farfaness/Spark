# -*- coding: utf-8 -*-
"""Dataframe_Databricks_Community_Edition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17hRp_qb1kDeTeyb0e_4DV0K57g4tuooX
"""

from pyspark.sql import SparkSession # Importer l'objet SparkSession (inutile dans le cas du Notebook Databricks)

# On instantie la SparkSession : cet objet est déja instantié dans Databricks.
# Il va nous suivre tout au long de l'application
# Les options de configurations sont infinies : elles peuvent toucher le contexte d'execution, 
# le nombre de noeuds utilisés... 
# Ici nous précisons le minimum : Le nom de l'application et le master, c'est à dire l'allocateur de ressources.
# En disant local on va utiliser l'allocateur de ressources interne à Spark : Spark Standalone. 
# Si vous lancez votre job Spark sur un cluster Hadoop, vous pouvez utiliser yarn
spark = SparkSession.builder.appName("First App").master("local").getOrCreate()

# Le sparkContext est l'outil qui permet d'explorer le plus bas niveau API de spark : la RDD
sc = spark.sparkContext

"""#### Charger la donnée"""

bateaux = spark.read.parquet("/FileStore/tables/2c443e0c8b99190f_c98fa37700000000_457998991_data_0__1__1_-c0514.parq")

bateaux.show()

"""#### Afficher le nombre d'éléments du DF"""

bateaux.count()

"""#### Afficher le type des éléments du DF"""

bateaux.schema

"""#### Afficher le schéma du DF"""

bateaux.printSchema()

"""#### Ordonnez les bateaux par vitesse"""

# Rappel : on ne peut modifier un RDD, ni une DataFrame. Ici nous avons donc créer une nouvelle DF et détruit l'ancienne
# Si on veut garder la trace de l'ancienne il est important de charger la nouvelle dans une autre variable
bateaux = bateaux.orderBy("speed")

"""#### Pour disposer d'une date plus lisible, rajoutez la colonne "timestamp_readable"
#### Pour cela, il vous faudra d'abord transformer le timestamp en secondes (timestamp_pos est en ms) et ensuite convertir en format date
"""

bateaux_date = bateaux.withColumn("timestamp_readable", (bateaux["timestamp_pos"]/1000).cast("timestamp"))

bateaux_date.show()

"""#### Il est possible de créer un nouveau dataframe contenant des statistiques sur un dataframe"""

description_df = bateaux.describe()
description_df.show()

"""#### Filtrer le dataframe bateaux sur un bateau en particulier"""

monBateau = bateaux.filter("id_vessel = 28052")
monBateau.show()

"""#### Le nombre de bateaux avec une vitesse == 0 et la fraction que cela représente sur par rapport au nombre total

#### Les types distincts de bateaux
"""

bateaux[["type"]].dropDuplicates().show()

"""#### On affiche la moyenne de la vitesse des bateaux sans tenir compte des bateaux a l'arret"""

enmarche = bateaux.filter("speed !=0 ").groupBy("id_vessel").mean("speed")

enmarche.show()

"""#### On affiche la date min puis  la date max pour chaque bateau"""

bateaux_date.groupBy("id_vessel").agg({"timestamp_readable": "min"}).show()

bateaux_date.groupBy("id_vessel").agg({"timestamp_readable": "max"}).show()

"""#### Créer un dataframe avec l'id, la vitesse et le timestamp readable"""

spd = bateaux_date.select(["id_vessel","speed","timestamp_readable"])

"""#### Créer une colonne mois pour le mois en cours"""

from pyspark.sql.functions import month
spd = spd.withColumn("month", month(bateaux_date["timestamp_readable"]))
spd.show()

"""#### Ajouter une colonne boolean à True si le bateau est en mouvement"""

spd = spd.withColumn("BooleanCol", spd.speed != 0)

"""#### Enregistrer la table"""

bateaux.createOrReplaceTempView("tab_bateaux")

"""#### Afficher par type, le nombre de bateaux, la vitesse max et la moyenne de la vitesse, ordonnés par nombre de bateaux (requete SQL)"""

spark.sql("""SELECT type, max(speed), avg(speed), count(*) as num 
FROM tab_bateaux 
GROUP BY type 
ORDER BY count(*)""").show()

"""### Mettons nos compétences au service d'un cas d'usage un peu plus intéressant.
Nous allons prendre un bateau de notre choix et nous allons tenter de calculer quels sont les bateaux qu'il a pu croiser sur sa route

#### Créer un nouveau dataframe basé sur le DF bateaux et filtré uniquement sur id_vessel de votre choix.
#### Débarassez vous également de la colonne "payload" avec la méthode df.drop("colonne")
"""

monBateau.show()

monBateau = monBateau.drop('payload')
monBateau.show()

"""#### Créer un deuxième dataframe basé sur le DF bateaux, avec les colonnes id_vessel, lat, lon et timestamp_pos
#### On transforme nos DF en rdd grâce à la fonction rdd (pas besoin)
"""

bateaux2 = bateaux[["id_vessel", "lat", "lon", "timestamp_pos"]]
bateaux2.show()

"""#### On calcule le nombre d'enregistrements associés à notre bateau
 
#### On calcule le nombre d'enregistrements du RDD de référence
"""

print(monBateau.count(), bateaux2.count())
print((monBateau.count() * bateaux2.count()))

"""##### Créer un RDD batEtude produit cartesien de notre RDD bateau et du RDD référentiel"""

from pyspark.sql.functions import col

monBateau = monBateau.select(col("id_vessel").alias("my_id_vessel"), col("lat").alias("my_lat"), col("lon").alias("my_lon"), col("timestamp_pos").alias("my_timestamp_pos"))
monBateau.show()

batEtude = monBateau.crossJoin(bateaux2)
batEtude.count()

"""##### On affiche la première ligne pour analyser son contenu
##### Astuce : n'oubliez pas que l'on est maintenant sur des RDD
"""

batEtude.take(1)

"""##### On pourrait s'amuser à compter le nombre de lignes totales du RDD.. mais c'est une action !
##### Il est possible que cela prenne un peu de temps selon la taille de votre RDD contenant un seul id_vessel

##### B4 - Filtrer notre produit cartésien pour éliminer les bateaux identiques à notre id_vessel choisi
##### Astuce : notre RDD batEtude est composé de tuples(bateau,bateauRef)
"""

batEtude.columns

filtered = batEtude.filter(batEtude.id_vessel != batEtude.my_id_vessel)
filtered.count()

"""##### On va également filtrer les informations des bateaux pour ne conserver que les enregistrements à +/- une heure (3600s)"""

#heure = 1416007969000
filtered2 = filtered.filter((filtered.my_timestamp_pos <= (filtered.timestamp_pos + 3600)) & (filtered.my_timestamp_pos >= (filtered.timestamp_pos - 3600)))
filtered2.count()

#filtered2 = filtered.filter(abs(filtered.my_timestamp_pos - filtered.timestamp_pos) < 3600)
#filtered2.count()

from pyspark.sql import functions as F
from pyspark.sql.types import LongType
@F.udf(returnType=LongType())
def calculateDistanceInKilometer(userlat, userlon, otherlat, otherlon):
    AVERAGE_RADIUS_OF_EARTH_KM = 6371
    latDistance = math.radians(userlat - otherlat)
    lngDistance = math.radians(userlon - otherlon)
    sinLat = math.sin(latDistance / 2)
    sinLng = math.sin(lngDistance / 2)
    a = sinLat * sinLat + (math.cos(math.radians(userlat)) * math.cos(math.radians(otherlat))* sinLng * sinLng)
    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
    return int(AVERAGE_RADIUS_OF_EARTH_KM * c)

"""### Filtrer notre RDD en ne conservant que les bateaux pour lesquels la distance est inférieure à 1 km
La méthode calculateDistanceInKilometer(lat,lon,lat,long) renvoi la distance en kilomètres entre deux coordonnées
"""

import math
import numpy as np
#filtered3 = filtered2.fillna(float(np.NaN))
filtered3 = filtered2.dropna()
filtered3.show()

filtered3.dtypes

from pyspark.sql.types import FloatType
cols = ['my_lat', 'my_lon', 'lat', 'lon']

dataframe_temp = filtered3

for col_name in cols:
  dataframe_temp = dataframe_temp.withColumn(col_name, col(col_name).cast('float'))

dataframe_temp.dtypes

from pyspark.sql.functions import udf
from pyspark.sql.types import FloatType, ArrayType
filtered4 = dataframe_temp.filter(calculateDistanceInKilometer(dataframe_temp.my_lat, dataframe_temp.my_lon, dataframe_temp.lat, dataframe_temp.lon) < 1.0)
filtered4.show()